{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QTKdz07Q-Unk"
      },
      "outputs": [],
      "source": [
        "!wget"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import RobertaForSequenceClassification, RobertaTokenizer, RobertaConfig, AdamW, get_linear_schedule_with_warmup\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "\n",
        "# If you are using colab\n",
        "drive.mount(\"/content/Drive/\")\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv('/content/Drive/My Drive/ILDC_multi.csv')  # path to multi_dataset\n",
        "\n"
      ],
      "metadata": {
        "id": "_CT33PB44B-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#n1 = df.shape[0]\n",
        "#test_n = int(0.1*n1)\n",
        "#test_n\n",
        "#new_df = df[:test_n]\n",
        "subset_df = df.sample(frac=0.05, random_state=42)\n",
        "\n",
        "# Split the subset into train, test, and validation sets based on the 'split' column\n",
        "train_set, test_and_val_set = train_test_split(subset_df, test_size=0.4, random_state=42)\n",
        "test_set, validation_set = train_test_split(test_and_val_set, test_size=0.5, random_state=42)\n"
      ],
      "metadata": {
        "id": "VrRiJOXT-TaE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "qzGDTdJLIsJf",
        "outputId": "ab078232-5a70-4067-e913-2214221c5c58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                    text  label  split  \\\n",
              "34165  \\nshah j. \\n\\na deed of partnership for carryi...      1   test   \n",
              "13950   B. Pattanaik and B. N. Agrawal, JJ. This appe...      0  train   \n",
              "14074   2001  3  SCR 424 The following Orders of the ...      0  train   \n",
              "23228   Varadarajan, J. These Criminal Appeals by spe...      1  train   \n",
              "6876    Dr. ARIJIT PASAYAT, J. Noticing that there we...      1  train   \n",
              "...                                                  ...    ...    ...   \n",
              "5045   Leave granted. This appeal is directed against...      1  train   \n",
              "21402  CIVIL APPELLATE JURISDICTION Civil Appeal No. ...      0  train   \n",
              "9419    CRIMINAL APPEAL NO. 548 OF 2007 P. MATHUR, J....      1  train   \n",
              "14275   Heard the learned Counsel for the parties. Le...      1  train   \n",
              "14844   SETHI,J. Leave granted. LITTTTTTTJ Respondent...      1  train   \n",
              "\n",
              "               name  \n",
              "34165  1967_187.txt  \n",
              "13950  2001_171.txt  \n",
              "14074  2001_370.txt  \n",
              "23228   1983_96.txt  \n",
              "6876   2009_391.txt  \n",
              "...             ...  \n",
              "5045   2011_770.txt  \n",
              "21402  1981_240.txt  \n",
              "9419   2007_968.txt  \n",
              "14275  2001_696.txt  \n",
              "14844  2000_840.txt  \n",
              "\n",
              "[1044 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1e28b3c3-4225-4320-9912-8aab336c6529\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>split</th>\n",
              "      <th>name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>34165</th>\n",
              "      <td>\\nshah j. \\n\\na deed of partnership for carryi...</td>\n",
              "      <td>1</td>\n",
              "      <td>test</td>\n",
              "      <td>1967_187.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13950</th>\n",
              "      <td>B. Pattanaik and B. N. Agrawal, JJ. This appe...</td>\n",
              "      <td>0</td>\n",
              "      <td>train</td>\n",
              "      <td>2001_171.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14074</th>\n",
              "      <td>2001  3  SCR 424 The following Orders of the ...</td>\n",
              "      <td>0</td>\n",
              "      <td>train</td>\n",
              "      <td>2001_370.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23228</th>\n",
              "      <td>Varadarajan, J. These Criminal Appeals by spe...</td>\n",
              "      <td>1</td>\n",
              "      <td>train</td>\n",
              "      <td>1983_96.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6876</th>\n",
              "      <td>Dr. ARIJIT PASAYAT, J. Noticing that there we...</td>\n",
              "      <td>1</td>\n",
              "      <td>train</td>\n",
              "      <td>2009_391.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5045</th>\n",
              "      <td>Leave granted. This appeal is directed against...</td>\n",
              "      <td>1</td>\n",
              "      <td>train</td>\n",
              "      <td>2011_770.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21402</th>\n",
              "      <td>CIVIL APPELLATE JURISDICTION Civil Appeal No. ...</td>\n",
              "      <td>0</td>\n",
              "      <td>train</td>\n",
              "      <td>1981_240.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9419</th>\n",
              "      <td>CRIMINAL APPEAL NO. 548 OF 2007 P. MATHUR, J....</td>\n",
              "      <td>1</td>\n",
              "      <td>train</td>\n",
              "      <td>2007_968.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14275</th>\n",
              "      <td>Heard the learned Counsel for the parties. Le...</td>\n",
              "      <td>1</td>\n",
              "      <td>train</td>\n",
              "      <td>2001_696.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14844</th>\n",
              "      <td>SETHI,J. Leave granted. LITTTTTTTJ Respondent...</td>\n",
              "      <td>1</td>\n",
              "      <td>train</td>\n",
              "      <td>2000_840.txt</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1044 rows Ã— 4 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1e28b3c3-4225-4320-9912-8aab336c6529')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1e28b3c3-4225-4320-9912-8aab336c6529 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1e28b3c3-4225-4320-9912-8aab336c6529');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-bf1635f4-af56-4af7-bb13-3051bcb86813\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-bf1635f4-af56-4af7-bb13-3051bcb86813')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-bf1635f4-af56-4af7-bb13-3051bcb86813 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'roberta-base'\n",
        "tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
        "\n",
        "def pad_sequences(sequences, maxlen=None, dtype=torch.long, padding='pre', truncating='pre', value=0.):\n",
        "    # If maxlen is not provided, infer it from the longest sequence\n",
        "    if maxlen is None:\n",
        "        maxlen = max(len(seq) for seq in sequences)\n",
        "\n",
        "    # Initialize the padded sequences tensor with zeros\n",
        "    padded_sequences = []\n",
        "\n",
        "    for seq in sequences:\n",
        "        if truncating == 'pre':\n",
        "            truncated_seq = seq[-maxlen:]\n",
        "        else:\n",
        "            truncated_seq = seq[:maxlen]\n",
        "\n",
        "        if padding == 'pre':\n",
        "            padded_seq = [value] * (maxlen - len(truncated_seq)) + truncated_seq\n",
        "        else:\n",
        "            padded_seq = truncated_seq + [value] * (maxlen - len(truncated_seq))\n",
        "\n",
        "        padded_sequences.append(padded_seq)\n",
        "\n",
        "    return torch.tensor(padded_sequences, dtype=dtype)\n",
        "\n",
        "def input_id_maker(dataf, tokenizer):\n",
        "    input_ids = []\n",
        "    lengths = []\n",
        "\n",
        "    for i in tqdm(range(len(dataf['text']))):\n",
        "        sen = dataf['text'].iloc[i]\n",
        "        sen = tokenizer.tokenize(sen, add_prefix_space=True)\n",
        "        CLS = tokenizer.cls_token\n",
        "        SEP = tokenizer.sep_token\n",
        "        if(len(sen) > 510):\n",
        "            sen = sen[len(sen)-510:]\n",
        "\n",
        "        sen = [CLS] + sen + [SEP]\n",
        "        encoded_sent = tokenizer.convert_tokens_to_ids(sen)\n",
        "        input_ids.append(encoded_sent)\n",
        "        lengths.append(len(encoded_sent))\n",
        "\n",
        "    input_ids = pad_sequences(input_ids, maxlen=512, value=0, dtype=torch.long, truncating=\"pre\", padding=\"post\")\n",
        "    return input_ids, lengths\n",
        "\n",
        "train_input_ids, train_lengths = input_id_maker(train_set, tokenizer)\n",
        "validation_input_ids, validation_lengths = input_id_maker(validation_set, tokenizer)\n",
        "\n",
        "def att_masking(input_ids):\n",
        "    attention_masks = []\n",
        "    for sent in input_ids:\n",
        "        att_mask = [int(token_id > 0) for token_id in sent]\n",
        "        attention_masks.append(att_mask)\n",
        "    return attention_masks\n",
        "\n",
        "train_attention_masks = att_masking(train_input_ids)\n",
        "validation_attention_masks = att_masking(validation_input_ids)\n",
        "\n",
        "train_labels = train_set['label'].to_numpy().astype('int')\n",
        "validation_labels = validation_set['label'].to_numpy().astype('int')\n",
        "\n",
        "train_inputs = torch.tensor(train_input_ids)\n",
        "train_labels = torch.tensor(train_labels)\n",
        "train_masks = torch.tensor(train_attention_masks)\n",
        "\n",
        "validation_inputs = torch.tensor(validation_input_ids)\n",
        "validation_labels = torch.tensor(validation_labels)\n",
        "validation_masks = torch.tensor(validation_attention_masks)\n",
        "\n",
        "# Set batch size\n",
        "batch_size = 6\n",
        "\n",
        "# Create DataLoader for training\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# Create DataLoader for validation\n",
        "if len(validation_inputs) > 0:\n",
        "    # Create DataLoader for validation\n",
        "    validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "    validation_sampler = RandomSampler(validation_data)\n",
        "    validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
        "else:\n",
        "    print(\"Validation set is empty.\")\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Load Roberta model for sequence classification\n",
        "model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2)\n",
        "model.to(device)\n",
        "\n",
        "lr = 2e-6\n",
        "epochs = 3\n",
        "num_total_steps = len(train_dataloader) * epochs\n",
        "num_warmup_steps = 1000\n",
        "\n",
        "# Create optimizer and scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=lr, correct_bias=True)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_total_steps)\n",
        "\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "seed_val = 2212\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "loss_values = []\n",
        "\n",
        "# Training loop\n",
        "for epoch_i in range(epochs):\n",
        "    print(f'======== Epoch {epoch_i + 1} / {epochs} ========')\n",
        "    print('Training...')\n",
        "\n",
        "    total_loss = 0\n",
        "    model.train()\n",
        "\n",
        "    for step, batch in enumerate(tqdm(train_dataloader)):\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        model.zero_grad()\n",
        "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
        "        loss = outputs[0]\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_dataloader)\n",
        "    loss_values.append(avg_train_loss)\n",
        "\n",
        "    print(f\"\\nAverage training loss: {avg_train_loss:.2f}\")\n",
        "\n",
        "    print(\"\\nRunning Validation...\")\n",
        "    model.eval()\n",
        "\n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "    for batch in tqdm(validation_dataloader):\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "\n",
        "        logits = outputs[0]\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "    print(f\"\\nAccuracy: {eval_accuracy/nb_eval_steps:.2f}\")\n",
        "\n",
        "print(\"\\nTraining complete!\")\n",
        "\n",
        "# Save the trained model\n",
        "output_dir = './RoBERTa_final/'\n",
        "\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "print(f\"Saving model to {output_dir}\")\n",
        "model_to_save = model.module if hasattr(model, 'module') else model\n",
        "model_to_save.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "# Copy the model files to a directory in your Google Drive.\n",
        "!cp -r ./RoBERTa_final2/ \"/content/Drive/My Drive/RoBERTa_right_model2/\"\n",
        "\n",
        "# Testing the model\n",
        "labels = test_set.label.to_numpy().astype(int)\n",
        "\n",
        "input_ids, input_lengths = input_id_maker(test_set, tokenizer)\n",
        "attention_masks = att_masking(input_ids)\n",
        "\n",
        "# Convert to tensors.\n",
        "prediction_inputs = torch.tensor(input_ids)\n",
        "prediction_masks = torch.tensor(attention_masks)\n",
        "prediction_labels = torch.tensor(labels)\n",
        "\n",
        "# Set the batch size.\n",
        "batch_size = 6\n",
        "\n",
        "# Create the DataLoader.\n",
        "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)\n",
        "\n",
        "print(f'Predicting labels for {len(prediction_inputs):,} test sentences...')\n",
        "model.eval()\n",
        "\n",
        "predictions, true_labels = [], []\n",
        "\n",
        "for (step, batch) in enumerate(tqdm(prediction_dataloader)):\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "\n",
        "    logits = outputs[0]\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "    predictions.append(logits)\n",
        "    true_labels.append(label_ids)\n",
        "\n",
        "print('    DONE.')\n",
        "\n",
        "predictions = np.concatenate(predictions, axis=0)\n",
        "true_labels = np.concatenate(true_labels, axis=0)\n",
        "\n",
        "pred_flat = np.argmax(predictions, axis=1).flatten()\n",
        "labels_flat = true_labels.flatten()\n",
        "\n",
        "accuracy = flat_accuracy(predictions, true_labels)\n",
        "print(f'Accuracy: {accuracy:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbXZJjPI9wYd",
        "outputId": "a2879caa-e8db-4726-9a77-6d011c1021f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1044/1044 [00:23<00:00, 43.76it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 349/349 [00:07<00:00, 44.44it/s]\n",
            "<ipython-input-37-c2bedebc8589>:63: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_inputs = torch.tensor(train_input_ids)\n",
            "<ipython-input-37-c2bedebc8589>:67: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  validation_inputs = torch.tensor(validation_input_ids)\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======== Epoch 1 / 3 ========\n",
            "Training...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 174/174 [01:53<00:00,  1.54it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Average training loss: 0.70\n",
            "\n",
            "Running Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 59/59 [00:13<00:00,  4.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 0.38\n",
            "======== Epoch 2 / 3 ========\n",
            "Training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 174/174 [01:52<00:00,  1.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average training loss: 0.69\n",
            "\n",
            "Running Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 59/59 [00:13<00:00,  4.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 0.64\n",
            "======== Epoch 3 / 3 ========\n",
            "Training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 174/174 [01:52<00:00,  1.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average training loss: 0.69\n",
            "\n",
            "Running Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 59/59 [00:13<00:00,  4.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 0.61\n",
            "\n",
            "Training complete!\n",
            "Saving model to ./RoBERTa_final/\n",
            "cp: cannot stat './RoBERTa_final2/': No such file or directory\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 348/348 [00:05<00:00, 60.94it/s]\n",
            "<ipython-input-37-c2bedebc8589>:189: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prediction_inputs = torch.tensor(input_ids)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicting labels for 348 test sentences...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 58/58 [00:12<00:00,  4.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    DONE.\n",
            "Accuracy: 0.59\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# Zip the directory\n",
        "shutil.make_archive(\"/content/RoBERTa_final\", 'zip', \"/content/RoBERTa_final\")\n",
        "\n",
        "# Move the zipped file to Google Drive\n",
        "shutil.move(\"/content/RoBERTa_final.zip\", \"/content/Drive/My Drive/RoBERTa_final.zip\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "bZv2C5hMK6PP",
        "outputId": "90564b9c-498b-4a5a-946c-47c11aec6e22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/Drive/My Drive/RoBERTa_final.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tRlGDiXc7gNL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lNrP1wyn8sIf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}